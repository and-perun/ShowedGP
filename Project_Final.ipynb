{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f40955-6edc-4e56-b9e8-acc6cd70eace",
   "metadata": {},
   "source": [
    "## Reading Time Prediction from Syntactic Surprisal\n",
    "\n",
    "Main codebase of final project for Prof. Frank's LING 380 course\n",
    "\n",
    "Written by Mandy Osuji, Rishika Veeramachaneni, and Andrew Perun\n",
    "\n",
    "Requires CSV files relating to Filler.csv and ClassicGardenPathSet.csv, as processed by LING_380_Preprocess.ipynb, to be stored at paths data & data_GP\n",
    "\n",
    "For data availability, train the regressions using syntactic surprisals produced by all four of Arehalli's LMs, but separate/compare the predicted reading times by model.\n",
    "\n",
    "Note **minimal garbage collection takes place**; RAM bottlenecks must be assessed before running subsequent code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa3e80e-526b-4ca1-9295-7eb2fe9b2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import gc\n",
    "import psutil\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import fligner, ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from joblib import dump, load\n",
    "\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84773d9c-8210-489d-a653-bdeb8fd30ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your base file system, data will be written in made subdirectories\n",
    "\n",
    "base = \"/Users/andrewperun/Desktop/LING_380/Final_Proj/Trial2/\"\n",
    "\n",
    "gp_cases_base = base + \"single_GP\"\n",
    "os.makedirs(gp_cases_base, exist_ok=True)\n",
    "\n",
    "prediction_base = base + \"predictions\"\n",
    "os.makedirs(prediction_base, exist_ok=True)\n",
    "\n",
    "exclude_base = base + \"exclude_GP\"\n",
    "os.makedirs(exclude_base, exist_ok=True)\n",
    "\n",
    "model_base = base + \"regressions\"\n",
    "os.makedirs(model_base, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e7120-0da8-4280-be88-d2e9bfa4b2bd",
   "metadata": {},
   "source": [
    "#### Function Declarations: Data Preprocessing\n",
    "\n",
    "To add lagged values, log frequencies/count, sentence position, and produce minimal dataframes for regression training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "578cfc8f-205c-4bbb-ac30-852b19980154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_with_progress(grouped, func):\n",
    "    \"\"\"\n",
    "    Applies a function to each group in a grouped DataFrame with progress tracking and memory optimization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    grouped : DataFrameGroupBy\n",
    "        The grouped DataFrame to process.\n",
    "    func : callable\n",
    "        Function to apply to each group.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Concatenated result of applying the function to all groups.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n",
    "        results.append(func(group))\n",
    "\n",
    "    print(\"Length of results: \" + str(len(results)))\n",
    "\n",
    "    for i, df in enumerate(results):\n",
    "        if df.empty:\n",
    "            print(\"Empty DataFrame at index \" + str(i)) # ensure all dfs nonempty\n",
    "\n",
    "    print(\"Concatenating into chunks...\")\n",
    "    chunk_size = 10000\n",
    "    concatenated_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(results), chunk_size), desc=\"Concatenating\", unit=\"chunk\"): # Chunk and concatenate\n",
    "        current_chunk = results[i:i + chunk_size]\n",
    "        if current_chunk:  # if chunk is not empty\n",
    "            # Only append non-empty DataFrames to concatenated_chunks\n",
    "            non_empty_chunks = [df for df in current_chunk if not df.empty]\n",
    "            if non_empty_chunks:\n",
    "                concatenated_chunks.append(pd.concat(non_empty_chunks))\n",
    "            results[i:i + chunk_size] = [None] * len(current_chunk) # 'safe' memory management (haha)\n",
    "\n",
    "    print(\"Length of concatenated chunks: \" + str(len(concatenated_chunks)))\n",
    "    for i in range(len(concatenated_chunks)):\n",
    "        print(concatenated_chunks[i].head())\n",
    "\n",
    "    print(f\"Memory usage before concat: {psutil.virtual_memory().used / 1e6:.2f} MB\") # memory update 1\n",
    "    result_df = pd.concat(concatenated_chunks, ignore_index=True)\n",
    "    print(f\"Memory usage after concat: {psutil.virtual_memory().used / 1e6:.2f} MB\")\n",
    "    gc.collect()  # this could be re-factored & pushed earlier, but it works\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def add_lags(group):\n",
    "    \"\"\"\n",
    "    Adds lagged columns (1, 2, 3 steps back) for lexical, syntactic, frequency, and length predictors to a group.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    group : DataFrame\n",
    "        A single group DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        The group with new lagged columns added.\n",
    "    \"\"\"\n",
    "    return group.assign(\n",
    "        lex_surprisal_p1_s=group['lex_surprisal_s'].shift(1),\n",
    "        lex_surprisal_p2_s=group['lex_surprisal_s'].shift(2),\n",
    "        lex_surprisal_p3_s=group['lex_surprisal_s'].shift(3),\n",
    "        syn_surprisal_p1_s=group['syn_surprisal_s'].shift(1),\n",
    "        syn_surprisal_p2_s=group['syn_surprisal_s'].shift(2),\n",
    "        syn_surprisal_p3_s=group['syn_surprisal_s'].shift(3),\n",
    "        logfreq_p1_s=group['logfreq_s'].shift(1),\n",
    "        logfreq_p2_s=group['logfreq_s'].shift(2),\n",
    "        logfreq_p3_s=group['logfreq_s'].shift(3),\n",
    "        length_p1_s=group['length_s'].shift(1),\n",
    "        length_p2_s=group['length_s'].shift(2),\n",
    "        length_p3_s=group['length_s'].shift(3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8d53d2b-2552-4d1a-b987-30a9d9e3cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_path, output_path, base_path):\n",
    "    \"\"\"\n",
    "    Processes a dataset by filtering, applying lagged predictors, and saving the cleaned output.\n",
    "\n",
    "    Steps:\n",
    "        Load and Filter Data: Read data from data_path and ensure no missing values in critical columns.\n",
    "        Apply Lags: Add lagged predictors for grouped data and optionally save the lagged dataset.\n",
    "        Process and Clean: Add sentence length, drop rows with missing predictors, and select essential columns.\n",
    "        Save Final Data: Write the cleaned dataset to output_path.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the input dataset CSV file.\n",
    "    output_path : str\n",
    "        Path to save the processed dataset.\n",
    "    save_lags : bool, optional\n",
    "        If True, saves the dataset with lagged predictors to `lags_path`. NOTE: This may cause excessive memory usage.\n",
    "    lags_path : str, optional\n",
    "        Path to save the lagged dataset if `save_lags=True`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves the processed data to the specified output path.\n",
    "    \"\"\"\n",
    "    merged = pd.read_csv(data_path, header=0)\n",
    "    #merged.head()\n",
    "    filtered_rows = merged[merged['logfreq_s'].isna() | merged['lex_surprisal_s'].isna() | merged['syn_surprisal_s'].isna() | merged['length_s'].isna() ]\n",
    "    if len(filtered_rows) > 0:\n",
    "        raise ValueError(\"Dataset has rows with missing values.\")\n",
    "    \n",
    "    merged_withlags = apply_with_progress(\n",
    "        merged.groupby(['item', 'MD5', 'model']),\n",
    "        add_lags\n",
    "    )\n",
    "    \n",
    "    merged_withlags.to_csv(os.path.join(base, \"with_lags.csv\"))\n",
    "\n",
    "    merged_withlags['sent_length'] = merged_withlags['Sentence'].str.split(\" \").apply(len)\n",
    "    \n",
    "    columns_to_check = [\n",
    "        'lex_surprisal_s', 'lex_surprisal_p1_s', 'lex_surprisal_p2_s', 'lex_surprisal_p3_s',\n",
    "        'syn_surprisal_s', 'syn_surprisal_p1_s', 'syn_surprisal_p2_s', 'syn_surprisal_p3_s',\n",
    "        'logfreq_s', 'logfreq_p1_s', 'logfreq_p2_s', 'logfreq_p3_s'\n",
    "    ]\n",
    "\n",
    "    dropped = merged_withlags.dropna(subset=columns_to_check)\n",
    "\n",
    "    columns_to_keep = [\n",
    "        \"RT\", \"syn_surprisal_s\", \"syn_surprisal_p1_s\", \"syn_surprisal_p2_s\",\n",
    "        \"lex_surprisal_s\", \"lex_surprisal_p1_s\", \"lex_surprisal_p2_s\",\n",
    "        \"WordPosition\", \"logfreq_s\", \"logfreq_p1_s\", \"logfreq_p2_s\",\n",
    "        \"length_s\", \"length_p1_s\", \"length_p2_s\", \"MD5\", \"item\", \"model\"\n",
    "    ]\n",
    "    dropped_minimal = dropped[columns_to_keep]\n",
    "    print(\"Data writen out to \" + output_path) \n",
    "    dropped_minimal.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368a777-e813-41c2-bb1e-68eff69a35c4",
   "metadata": {},
   "source": [
    "## Processing filler & GP surprisal data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "604f170f-3adb-41f9-b9e8-2b96b0b8d359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = base + \"fmerged_mod.csv\"\n",
    "out = base + \"fdropped_minimal.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30124d46-231e-4637-8005-d6b4b9469f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(data, out, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7b749db-0f6e-47ca-a527-53d26fbdfce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_GP = base + \"merged_mod.csv\"\n",
    "out_GP = base + \"dropped_minimal.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458937b-a83f-41cc-b40a-f2a5029454e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(data_GP, out_GP, base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e48bdc-e7bf-4f79-8a23-9fa61f13ddb6",
   "metadata": {},
   "source": [
    "## Statistical Testing:\n",
    "\n",
    "Comparing syntactic surprisal and reading times between regression-trained (only filler) and predictive (garden path case) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be634ddd-326b-4b38-bd08-e9031c25b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_KS_FK(set1, set2):\n",
    "    # KS test\n",
    "    stat_KS, p_value_KS = ks_2samp(set1, set2)\n",
    "    print(\"K-S Statistic:\", stat_KS)\n",
    "    print(\"P-value:\", p_value_KS)\n",
    "    if p_value_KS < 0.05:\n",
    "        print(\"Distributions significantly different.\")\n",
    "    else:\n",
    "        print(\"Distributions not significantly different.\")\n",
    "\n",
    "    # FK test\n",
    "    stat_FK, p_value_FK = fligner(set1, set2)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Fligner-Killeen Statistic:\", stat_FK)\n",
    "    print(\"P-value:\", p_value_FK)\n",
    "    \n",
    "    # Interpretation\n",
    "    if p_value_FK < 0.05:\n",
    "        print(\"Variances significantly different.\")\n",
    "    else:\n",
    "        print(\"Variances not significantly different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d84d9-0526-4cdc-bc26-9d9d73642bed",
   "metadata": {},
   "source": [
    "##### Testing Syntactic Surprisal of Filler and All GP Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76697c29-ea86-4767-b9af-7423a33e469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_lags = pd.read_csv(os.path.join(base, \"with_lags.csv\"), header=0)\n",
    "fdropped = pd.read_csv(out_GP, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d231911e-65dc-4436-ac57-dc4febf846ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsurp_GP = list(with_lags[\"syn_surprisal_s\"])\n",
    "synsurp_F  = list(fdropped[\"syn_surprisal_s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ad419775-3e37-4c7d-acb8-c7b89d9c2c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-S Statistic: 0.07450928404435464\n",
      "P-value: 0.0\n",
      "Distributions significantly different.\n",
      "Fligner-Killeen Statistic: 11009.802752211754\n",
      "P-value: 0.0\n",
      "Variances significantly different.\n"
     ]
    }
   ],
   "source": [
    "test_KS_FK(synsurp_GP, synsurp_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1fc28a-632d-4051-9774-de5961247f83",
   "metadata": {},
   "source": [
    "##### Testing Syntactic Surprisal of Filler and Unambiguous GP Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "92eb966c-cd74-4898-853a-b07d2868b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsurp_Unam_GP = list(with_lags[with_lags['Type'].str.endswith('_UAMB', na=False)][\"syn_surprisal_s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a481d027-529c-4e47-b0ef-1aa76e1a2ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-S Statistic: 0.06634687648868465\n",
      "P-value: 0.0\n",
      "Distributions significantly different.\n",
      "Fligner-Killeen Statistic: 22101.98849543789\n",
      "P-value: 0.0\n",
      "Variances significantly different.\n"
     ]
    }
   ],
   "source": [
    "test_KS_FK(synsurp_Unam_GP, synsurp_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c330f7d-d178-4ac8-9fa2-eb6359997955",
   "metadata": {},
   "source": [
    "##### Testing Syntactic Surprisal of ambiguous GP cases against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d2412f-51d7-4e79-ab73-6ac59be80ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsurp_am_MVRR = list(with_lags[with_lags['Type'].str.endswith('MVRR_AMB', na=False)][\"syn_surprisal_s\"])\n",
    "synsurp_am_NPS  = list(with_lags[with_lags['Type'].str.endswith('NPS_AMB', na=False)][\"syn_surprisal_s\"])\n",
    "synsurp_am_NPZ =  list(with_lags[with_lags['Type'].str.endswith('NPZ_AMB', na=False)][\"syn_surprisal_s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98c8bb06-4d86-45cd-b163-359a1ba43333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 0 and 1\n",
      "K-S Statistic: 0.05195633602969618\n",
      "P-value: 0.0\n",
      "Distributions significantly different.\n",
      "Fligner-Killeen Statistic: 942.4621828964345\n",
      "P-value: 5.7714612682572204e-207\n",
      "Variances significantly different.\n",
      "Testing 0 and 2\n",
      "K-S Statistic: 0.09781969997807372\n",
      "P-value: 0.0\n",
      "Distributions significantly different.\n",
      "Fligner-Killeen Statistic: 13534.869266336262\n",
      "P-value: 0.0\n",
      "Variances significantly different.\n",
      "Testing 1 and 2\n",
      "K-S Statistic: 0.10507654229120222\n",
      "P-value: 0.0\n",
      "Distributions significantly different.\n",
      "Fligner-Killeen Statistic: 22489.51095001312\n",
      "P-value: 0.0\n",
      "Variances significantly different.\n"
     ]
    }
   ],
   "source": [
    "elems = [synsurp_am_MVRR, synsurp_am_NPS, synsurp_am_NPZ]\n",
    "for i in list(itertools.combinations({0, 1, 2}, 2)):\n",
    "    print(f\"Testing {i[0]} and {i[1]}\")\n",
    "    test_KS_FK(elems[i[0]], elems[i[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89cddb0-a25d-4569-aee6-02fd093fbf00",
   "metadata": {},
   "source": [
    "## Experiment 1:\n",
    "\n",
    "Training regressions using syntactic surprisal on filler reading times + {2 GP cases}, to produce reading times for the third GP case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8881ff10-650e-4fad-af64-95300a57bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fillerGP(mindata_filler, mindata_GP, output_path_base): # use the outputs from process_data\n",
    "    \"\"\"\n",
    "    Filters and combines datasets based on specific case exclusions and saves the results.\n",
    "\n",
    "    Steps:\n",
    "        1. Load Data: Reads the minimal filler dataset and the GP dataset.\n",
    "        2. Filter GP Data: Retains rows in the GP dataset for specific 'Type' cases ('_AMB' and '_UAMB' for MVRR, NPS, NPZ).\n",
    "        3. Exclude Cases: Iterates over the main cases (MVRR, NPS, NPZ), excluding one case at a time.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mindata_filler : str\n",
    "        Path to the CSV file containing the filler dataset.\n",
    "    mindata_GP : str\n",
    "        Path to the CSV file containing the GP dataset.\n",
    "    output_path_base : str\n",
    "        Directory path to save the resulting combined datasets.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Saves the combined datasets with excluded cases to the specified directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    fdropped_min = pd.read_csv(mindata_filler, header=0)\n",
    "    dropped_min = pd.read_csv(mindata_GP, header=0)\n",
    "\n",
    "    all_cases = list(set(list(dropped_min['Type'])))\n",
    "    gpath_cases = ['MVRR', 'NPS', 'NPZ']\n",
    "    gpath_cases_full = [c + '_AMB' for c in gpath_cases] + [c + '_UAMB' for c in gpath_cases];\n",
    "    \n",
    "    dropped_min = dropped_min[dropped_min['Type'].isin([c + '_AMB' for c in gpath_cases] + [c + '_UAMB' for c in gpath_cases])]\n",
    "    \n",
    "    for exclude_case in gpath_cases:\n",
    "        used_cases = [case for case in all_cases if case not in [exclude_case + '_AMB', exclude_case + '_UAMB']]\n",
    "    \n",
    "        filtered_rows = dropped_min[dropped_min['Type'].isin(used_cases)] # extract rows with those with cases in in_cases\n",
    "        df_combined = pd.concat([fdropped_min, filtered_rows], ignore_index=True, join='outer')\n",
    "        output_file = os.path.join(output_path_base, f\"minimal_-{exclude_case}.csv\")\n",
    "        print(\"Data writen out to \" + output_file)\n",
    "        df_combined.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a0204d9-652a-4fc4-8c8c-ab50d22a76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_regression(model_string, random_effect_string, dataset_path, model_out_path, optimizer = \"bfgs\"):\n",
    "\n",
    "    dataset = pd.read_csv(dataset_path, header = 0)\n",
    "\n",
    "    # Check consistent typing of all relevant columns\n",
    "    cols = dataset.columns[:15]\n",
    "    for col in cols:\n",
    "        column_types = dataset[col].apply(type)\n",
    "        if (column_types.nunique() != 1):\n",
    "            print(\"Column \" + str(col) + \" inconsistent.\") \n",
    "    \n",
    "    filler_model_syn = smf.mixedlm(\n",
    "        model_string,\n",
    "        data=dataset,\n",
    "        groups=dataset[\"item\"],  \n",
    "        re_formula=random_effect_string\n",
    "    ).fit(method=optimizer, maxiter=100000, disp=True) \n",
    "\n",
    "    dump(filler_model_syn, model_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6eb5b9f1-c28a-4920-9174-dc2852b0f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRT(model_string, model_path, dataset_path):\n",
    "    regression = load(model_path)\n",
    "    new_data = pd.read_csv(dataset_path, header=0)\n",
    "\n",
    "    fe_matrix = patsy.dmatrix( # retrieve fixed effects matrix using new_data\n",
    "        model_string.split(\"~\")[1], new_data, return_type=\"dataframe\"\n",
    "    )\n",
    "    fe_params = regression.fe_params # predicting fixed effects\n",
    "    new_data[\"fe_prediction\"] = fe_matrix @ fe_params\n",
    "\n",
    "    rand_effects = regression.random_effects # creating random effect matrix, filling na with 0\n",
    "    new_data[\"rand_effects\"] = new_data[\"item\"].map(rand_effects).fillna(0)\n",
    "    \n",
    "    new_data[\"predicted_rt_syn\"] = new_data[\"fe_prediction\"] + new_data[\"rand_effects\"] # get final RT predictions\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "135769a7-be24-4d62-a430-733e2f650757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRT_dataset(model_string, model_path, dataset, output_path):\n",
    "    regression = load(model_path)\n",
    "\n",
    "    fe_matrix = patsy.dmatrix( # retrieve fixed effects matrix using new_data\n",
    "        model_string.split(\"~\")[1], dataset, return_type=\"dataframe\"\n",
    "    )\n",
    "    fe_params = regression.fe_params # predicting fixed effects\n",
    "    dataset[\"fe_prediction\"] = fe_matrix @ fe_params\n",
    "\n",
    "    rand_effects = regression.random_effects # creating random effect matrix, filling na with 0\n",
    "    dataset[\"rand_effects\"] = dataset[\"item\"].map(rand_effects).fillna(0)\n",
    "    \n",
    "    dataset[\"predicted_rt_syn\"] = dataset[\"fe_prediction\"] + dataset[\"rand_effects\"] # get final RT predictions\n",
    "\n",
    "    dataset.to_csv(output_path, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81170b7-ab00-4744-b313-8bab8109f645",
   "metadata": {},
   "source": [
    "##### Getting Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6dfd3e-3a54-452f-93d6-f66d85aef761",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fillerGP(out, out_GP, exclude_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa4598-331c-4a3e-a683-fbc3c94b7e55",
   "metadata": {},
   "source": [
    "##### Fitting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e134b74-0f76-4522-bae3-c152b4b5f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_model = \"\"\"\n",
    "    RT ~ syn_surprisal_s + syn_surprisal_p1_s +\n",
    "         syn_surprisal_p2_s + scale(WordPosition) +\n",
    "         logfreq_s * length_s + logfreq_p1_s * length_p1_s +\n",
    "         logfreq_p2_s * length_p2_s\n",
    "    \"\"\"\n",
    "\n",
    "random_effect = \"1 + syn_surprisal_s + syn_surprisal_p1_s + syn_surprisal_p2_s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d440e0-efbb-4942-81de-400fee33d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_regression(syn_model, random_effect, \n",
    "               os.path.join(exclude_base, \"minimal_-MVRR.csv\"), \n",
    "               os.path.join(model_base, \"model-MVRR.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4dc5f-1ab8-4792-ab5b-897294088c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_regression(syn_model, random_effect, \n",
    "               os.path.join(exclude_base, \"minimal_-NPS.csv\"), \n",
    "               os.path.join(model_base, \"model-NPS.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159d0f4-6b04-4dee-9dd8-bedcf45729c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_regression(syn_model, random_effect, \n",
    "               os.path.join(exclude_base, \"minimal_-NPZ.csv\"), \n",
    "               os.path.join(model_base, \"model-NPZ.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4907f-ebe4-4cde-9d8f-d4983d65ecd8",
   "metadata": {},
   "source": [
    "##### Predicting Reading Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0aba6e37-df09-4563-a470-d29640c5d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min = pd.read_csv(out_GP, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf22b2-a5c2-495b-b15f-8905685085c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min_only_MVRR = dropped_min[dropped_min['Type'].isin(['MVRR_UAMB', 'MVRR_AMB'])]\n",
    "predictRT_dataset(syn_model, \n",
    "          os.path.join(model_base, \"model-MVRR.joblib\"), \n",
    "          dropped_min_only_MVRR,\n",
    "          os.path.join(prediction_base, \"MVRR_pred.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d193e80-ee44-4e0b-b5b3-4573cceb431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min_only_NPS = dropped_min[dropped_min['Type'].isin(['NPS_UAMB', 'NPS_AMB'])]\n",
    "predictRT_dataset(syn_model, \n",
    "          os.path.join(model_base, \"model-NPS.joblib\"), \n",
    "          dropped_min_only_NPS,\n",
    "          os.path.join(prediction_base, \"NPS_pred.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97ba38-7c6e-4690-a42e-321ea7800740",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min_only_NPZ = dropped_min[dropped_min['Type'].isin(['NPZ_UAMB', 'NPZ_AMB'])]\n",
    "predictRT_dataset(syn_model, \n",
    "          os.path.join(model_base, \"model-NPZ.joblib\"), \n",
    "          dropped_min_only_NPZ,\n",
    "          os.path.join(prediction_base, \"NPZ_pred.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dec546-fb88-43cd-aeab-e01de86fbfdf",
   "metadata": {},
   "source": [
    "##### Inspecting Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313d5fd3-e7e7-46e9-8264-ccac588af3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model(model_path):\n",
    "    print(f\"Model at {model_path}:\")\n",
    "    regression = load(model_path)\n",
    "    coeff = regression.params\n",
    "    #pval = filler_model_syn.pvalues\n",
    "    print(\"Coefficients:\\n\", coeff)\n",
    "    #print(\"P-values:\\n\", pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedd1933-230b-4d3b-aa03-44301ced333b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model at /Users/andrewperun/Desktop/LING_380/Final_Proj/Trial2/regressions/model-MVRR.joblib:\n",
      "Coefficients:\n",
      " Intercept                                      393.397933\n",
      "syn_surprisal_s                                 11.229539\n",
      "syn_surprisal_p1_s                               4.796593\n",
      "syn_surprisal_p2_s                              -2.083444\n",
      "scale(WordPosition)                             -0.690775\n",
      "logfreq_s                                      -19.568904\n",
      "length_s                                         8.743645\n",
      "logfreq_s:length_s                              -5.257586\n",
      "logfreq_p1_s                                   -16.766479\n",
      "length_p1_s                                      3.687154\n",
      "logfreq_p1_s:length_p1_s                        -3.889810\n",
      "logfreq_p2_s                                    -5.265493\n",
      "length_p2_s                                     -0.529202\n",
      "logfreq_p2_s:length_p2_s                        -0.926852\n",
      "Group Var                                        0.023498\n",
      "Group x syn_surprisal_s Cov                      0.005009\n",
      "syn_surprisal_s Var                              0.003324\n",
      "Group x syn_surprisal_p1_s Cov                   0.003195\n",
      "syn_surprisal_s x syn_surprisal_p1_s Cov         0.001037\n",
      "syn_surprisal_p1_s Var                           0.002261\n",
      "Group x syn_surprisal_p2_s Cov                   0.002767\n",
      "syn_surprisal_s x syn_surprisal_p2_s Cov         0.000721\n",
      "syn_surprisal_p1_s x syn_surprisal_p2_s Cov      0.000815\n",
      "syn_surprisal_p2_s Var                           0.001476\n",
      "dtype: float64\n",
      "Model at /Users/andrewperun/Desktop/LING_380/Final_Proj/Trial2/regressions/model-NPS.joblib:\n",
      "Coefficients:\n",
      " Intercept                                      394.855387\n",
      "syn_surprisal_s                                 11.774414\n",
      "syn_surprisal_p1_s                               5.855615\n",
      "syn_surprisal_p2_s                              -1.688277\n",
      "scale(WordPosition)                             -0.610356\n",
      "logfreq_s                                      -19.062679\n",
      "length_s                                        10.853661\n",
      "logfreq_s:length_s                              -4.626137\n",
      "logfreq_p1_s                                   -16.147750\n",
      "length_p1_s                                      5.657559\n",
      "logfreq_p1_s:length_p1_s                        -3.275663\n",
      "logfreq_p2_s                                    -5.171005\n",
      "length_p2_s                                     -0.223930\n",
      "logfreq_p2_s:length_p2_s                        -0.583581\n",
      "Group Var                                        0.022121\n",
      "Group x syn_surprisal_s Cov                      0.004796\n",
      "syn_surprisal_s Var                              0.003008\n",
      "Group x syn_surprisal_p1_s Cov                   0.003588\n",
      "syn_surprisal_s x syn_surprisal_p1_s Cov         0.001066\n",
      "syn_surprisal_p1_s Var                           0.002380\n",
      "Group x syn_surprisal_p2_s Cov                   0.002779\n",
      "syn_surprisal_s x syn_surprisal_p2_s Cov         0.000651\n",
      "syn_surprisal_p1_s x syn_surprisal_p2_s Cov      0.000936\n",
      "syn_surprisal_p2_s Var                           0.001614\n",
      "dtype: float64\n",
      "Model at /Users/andrewperun/Desktop/LING_380/Final_Proj/Trial2/regressions/model-NPZ.joblib:\n",
      "Coefficients:\n",
      " Intercept                                      392.963444\n",
      "syn_surprisal_s                                 10.470742\n",
      "syn_surprisal_p1_s                               4.280334\n",
      "syn_surprisal_p2_s                              -2.048860\n",
      "scale(WordPosition)                             -0.729539\n",
      "logfreq_s                                      -19.424525\n",
      "length_s                                         9.709022\n",
      "logfreq_s:length_s                              -4.926533\n",
      "logfreq_p1_s                                   -16.313927\n",
      "length_p1_s                                      4.916859\n",
      "logfreq_p1_s:length_p1_s                        -3.551668\n",
      "logfreq_p2_s                                    -5.268472\n",
      "length_p2_s                                     -0.263603\n",
      "logfreq_p2_s:length_p2_s                        -0.527860\n",
      "Group Var                                        0.023343\n",
      "Group x syn_surprisal_s Cov                      0.004497\n",
      "syn_surprisal_s Var                              0.003129\n",
      "Group x syn_surprisal_p1_s Cov                   0.002631\n",
      "syn_surprisal_s x syn_surprisal_p1_s Cov         0.000755\n",
      "syn_surprisal_p1_s Var                           0.002235\n",
      "Group x syn_surprisal_p2_s Cov                   0.002796\n",
      "syn_surprisal_s x syn_surprisal_p2_s Cov         0.000608\n",
      "syn_surprisal_p1_s x syn_surprisal_p2_s Cov      0.000871\n",
      "syn_surprisal_p2_s Var                           0.001861\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "inspect_model(os.path.join(model_base, \"model-MVRR.joblib\"))\n",
    "inspect_model(os.path.join(model_base, \"model-NPS.joblib\"))\n",
    "inspect_model(os.path.join(model_base, \"model-NPZ.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e24c6-9719-492c-852e-7b0927055ab3",
   "metadata": {},
   "source": [
    "## Computing GP Effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bee10b4-d05c-42c9-9f95-329ba6ecdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_predictions(preds_path):\n",
    "\n",
    "    header = [\n",
    "        \"idx\", \"RT\", \"Construction\", \"syn_surprisal_s\", \"syn_surprisal_p1_s\", \"syn_surprisal_p2_s\",\n",
    "        \"lex_surprisal_s\", \"lex_surprisal_p1_s\", \"lex_surprisal_p2_s\",\n",
    "        \"WordPosition\", \"logfreq_s\", \"logfreq_p1_s\", \"logfreq_p2_s\",\n",
    "        \"length_s\", \"length_p1_s\", \"length_p2_s\", \"MD5\", \"item\", \"model\", \"pred_RT\", \"eff1\", \"eff2\"\n",
    "    ]\n",
    "    \n",
    "    df = pd.read_csv(preds_path)\n",
    "    \n",
    "    items = df.iloc[:, -5].unique()  \n",
    "\n",
    "    separated_preds = {}\n",
    "\n",
    "    for value in items:\n",
    "        subset_df = df[df.iloc[:, -5] == value]\n",
    "        subset_df.columns = header\n",
    "        \n",
    "        amb_df = subset_df[subset_df['Construction'].str.endswith('_AMB', na=False)]\n",
    "        uamb_df = subset_df[subset_df['Construction'].str.endswith('_UAMB', na=False)]\n",
    "\n",
    "        # split by model as well\n",
    "        amb_split = {model: amb_df[amb_df['model'] == model] for model in amb_df['model'].unique()}\n",
    "        uamb_split = {model: uamb_df[uamb_df['model'] == model] for model in uamb_df['model'].unique()}\n",
    "\n",
    "\n",
    "        # Add to the dictionary\n",
    "        separated_preds[value] = {\n",
    "            'all': subset_df,\n",
    "            'amb': amb_df,\n",
    "            'uamb': uamb_df,\n",
    "            'amb_split': amb_split,\n",
    "            'uamb_split': uamb_split,\n",
    "        }\n",
    "\n",
    "    return separated_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f33780b0-3b70-4e6b-a183-cead939cc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropna_nestedDict(d, columns):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            #print(f\"Dropping in dict[{key}]\")\n",
    "            dropna_nestedDict(value, columns)\n",
    "        elif isinstance(value, pd.DataFrame):\n",
    "            d[key] = value.dropna(subset=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6c19ffa-5df3-4ab2-9b7d-17026a3e1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_Lags(lags_path, columns_to_check):\n",
    "\n",
    "    df = pd.read_csv(lags_path, header=0)\n",
    "    \n",
    "    items = df['item_x'].unique()  \n",
    "\n",
    "    separated_preds = {}\n",
    "\n",
    "    for value in items:\n",
    "        subset_df = df[df['item_x'] == value]\n",
    "        \n",
    "        mvrr = subset_df[subset_df['Type'].str.startswith('MVRR', na=False)]\n",
    "        nps = subset_df[subset_df['Type'].str.startswith('NPS', na=False)]\n",
    "        npz = subset_df[subset_df['Type'].str.startswith('NPZ', na=False)]\n",
    "        \n",
    "        # split by uamb/amb and model\n",
    "        mvrr_split = {\n",
    "            construction: {\n",
    "                model: mvrr[(mvrr['Type'] == construction) & (mvrr['model'] == model)]\n",
    "                for model in ['m0', 'm1', 'm2', 'm3']\n",
    "            }\n",
    "            for construction in mvrr['Type'].unique()\n",
    "        }\n",
    "\n",
    "        nps_split = {\n",
    "            construction: {\n",
    "                model: nps[(nps['Type'] == construction) & (nps['model'] == model)]\n",
    "                for model in ['m0', 'm1', 'm2', 'm3']\n",
    "            }\n",
    "            for construction in nps['Type'].unique()\n",
    "        }\n",
    "\n",
    "        npz_split = {\n",
    "            construction: {\n",
    "                model: npz[(npz['Type'] == construction) & (npz['model'] == model)]\n",
    "                for model in ['m0', 'm1', 'm2', 'm3']\n",
    "            }\n",
    "            for construction in npz['Type'].unique()\n",
    "        }\n",
    "    \n",
    "        # Add to the dictionary\n",
    "        separated_preds[value] = {\n",
    "            'mvrr': mvrr_split,\n",
    "            'nps': nps_split,\n",
    "            'npz': npz_split\n",
    "        }\n",
    "    \n",
    "    # Traverse all dictionaries and drop na values\n",
    "    dropna_nestedDict(separated_preds, columns_to_check)\n",
    "    \n",
    "    return separated_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67cde6dd-6d24-4c4d-844b-2b2842d094ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GPeffect_byModel(type_preds, lagged_truth, const):\n",
    "\n",
    "    # type_preds is a prediction dataframe\n",
    "    # lagged truth is lagged_sep\n",
    "    # const is one of ['mvrr', 'nps', 'npz']\n",
    "    \n",
    "    ##### Get human garden path effect #####\n",
    "    human_GP_effect = []\n",
    "    for i in lagged_truth.keys(): # for each sentence\n",
    "        unamb = lagged_truth[i][const][const.upper() + '_UAMB']['m0']\n",
    "        amb =   lagged_truth[i][const][const.upper() + '_AMB']['m0']\n",
    "\n",
    "        effect_list = []\n",
    "\n",
    "        # for each position around disambiguating word\n",
    "        for j in [-1, 0, 1, 2]:\n",
    "            df1 = unamb[unamb['ROI'] == j]['RT']\n",
    "            df2 = amb[amb['ROI'] == j]['RT']\n",
    "            min_len = min([len(df1), len(df2)])\n",
    "            \n",
    "            df1 = df1.reset_index(drop=True).iloc[:min_len]\n",
    "            df2 = df2.reset_index(drop=True).iloc[:min_len]\n",
    "\n",
    "            # get garden path effect at that position for that sentence, and append to list\n",
    "            diffs = (df2-df1).tolist()\n",
    "            effect_list.append(np.mean(diffs))\n",
    "\n",
    "        # append this sentences' GP effects\n",
    "        human_GP_effect.append(effect_list)\n",
    "\n",
    "    # now average GP effects through all sentences\n",
    "    human_GP_effect = zip(*human_GP_effect)\n",
    "    human_GP_effect = [sum(pos) / len(pos) for pos in human_GP_effect]\n",
    "\n",
    "\n",
    "    ##### Get Predicted path effect #####\n",
    "    GP_effects = []\n",
    "    for i in type_preds.keys(): # for each sentence\n",
    "        \n",
    "        i_list = []\n",
    "        \n",
    "        # get word position of disambiguating word\n",
    "        truth = lagged_truth[i][const][const.upper() + '_UAMB']['m0']\n",
    "        ambig_disambig = truth['disambPositionAmb'].iloc[0]\n",
    "        unambig_disambig = truth['disambPositionUnamb'].iloc[0]\n",
    "        #print(f\"Item {i}: {ambig_disambig}\")\n",
    "        #print(f\"Item {i}: {unambig_disambig}\")\n",
    "\n",
    "        \n",
    "        for model in ['m0', 'm1', 'm2', 'm3']: # for each model\n",
    "            \n",
    "            m_list = []\n",
    "            \n",
    "            unamb = type_preds[i]['uamb_split'][model]\n",
    "            amb = type_preds[i]['amb_split'][model]\n",
    "            \n",
    "            for j in [-1, 0, 1, 2]: # for each position around disambiguating word\n",
    "                df1 = unamb[unamb['WordPosition'] == j + unambig_disambig]['pred_RT']\n",
    "                df2 = amb[amb['WordPosition'] == j + ambig_disambig]['pred_RT']\n",
    "                min_len = min([len(df1), len(df2)])\n",
    "            \n",
    "                df1 = df1.reset_index(drop=True).iloc[:min_len]\n",
    "                df2 = df2.reset_index(drop=True).iloc[:min_len]\n",
    "\n",
    "                diffs = (df2-df1).tolist()\n",
    "                m_list.append(np.mean(diffs))\n",
    "\n",
    "            i_list.append(m_list)\n",
    "            \n",
    "        GP_effects.append(i_list)\n",
    "\n",
    "\n",
    "    trans = list(zip(*GP_effects))\n",
    "    means = []\n",
    "    for group in trans:\n",
    "        mean_sublist = np.mean(group, axis=0)\n",
    "        means.append(mean_sublist)\n",
    "\n",
    "    model_GP_effects = [list(sublist) for sublist in means]\n",
    "    \n",
    "    return human_GP_effect, model_GP_effects\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45893c62-44cf-45ed-8173-640605e48ec2",
   "metadata": {},
   "source": [
    "##### Getting GP effect from new RT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ed84fa1-b6a0-4d14-b104-fca597ab9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MVRR_preds = split_predictions(os.path.join(prediction_base, \"MVRR_pred.csv\"))\n",
    "NPS_preds  = split_predictions(os.path.join(prediction_base, \"NPS_pred.csv\"))\n",
    "NPZ_preds  = split_predictions(os.path.join(prediction_base, \"NPZ_pred.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "196972ae-6924-4bec-ba5f-c886037abbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "        'lex_surprisal_s', 'lex_surprisal_p1_s', 'lex_surprisal_p2_s', 'lex_surprisal_p3_s',\n",
    "        'syn_surprisal_s', 'syn_surprisal_p1_s', 'syn_surprisal_p2_s', 'syn_surprisal_p3_s',\n",
    "        'logfreq_s', 'logfreq_p1_s', 'logfreq_p2_s', 'logfreq_p3_s']\n",
    "\n",
    "lagged_sep = split_Lags(os.path.join(base, \"with_lags.csv\"), columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8b2a3a4b-03ee-412c-a037-182e0a3c7e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MVRR_human_GP, MVRR_model_GPs = get_GPeffect_byModel(MVRR_preds, lagged_sep, 'mvrr')\n",
    "NPS_human_GP,  NPS_model_GPs  = get_GPeffect_byModel(NPS_preds,  lagged_sep, 'nps')\n",
    "NPZ_human_GP,  NPZ_model_GPs  = get_GPeffect_byModel(NPZ_preds,  lagged_sep, 'npz')\n",
    "gpath_cases = ['MVRR', 'NPS', 'NPZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf0d95-f813-4732-9e32-7884940e957a",
   "metadata": {},
   "source": [
    "##### Inspecting and saving human & model GP effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "09a8e08f-fe85-4b7e-aa01-3edcc0f2dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.5711756860257005, 41.737653392294625, 167.4356757427677, 94.73673452609182]\n",
      "[17.100729397502587, 32.337040211112644, 49.61063704069727, 18.32235934651217]\n",
      "[5.757101983650362, 84.22053326940727, 119.79536401912738, 51.919479000115764]\n"
     ]
    }
   ],
   "source": [
    "print(MVRR_human_GP)\n",
    "print(NPS_human_GP)\n",
    "print(NPZ_human_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6774521c-deec-4243-95ce-67edd474f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"MVRR_human_GP\": MVRR_human_GP,\n",
    "    \"NPS_human_GP\": NPS_human_GP,\n",
    "    \"NPZ_human_GP\": NPZ_human_GP\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "path = os.path.join(base, \"Human_GPs.csv\")\n",
    "df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a0bf883-0b1b-4d7e-8e8e-5d1e4074f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.230610490613884, 17.186624745226453, 13.909817099294337, 2.2180659127159674], [-1.977681477480095, 12.984437937021061, 10.920510773908903, 1.7143983885677025], [-0.3013085566932446, 14.892478155166147, 10.116550408035563, 0.9995437859549909], [-5.242110117622686, 21.60587020400983, 17.71627788178112, 1.8141812661563808]]\n",
      "[[6.256116414395542, 11.630508235821075, 11.708929578695923, 3.170007797366162], [5.277581204845733, 12.583516500227669, 14.053373960941236, 4.0867581164792774], [5.244316622120549, 14.200459442989859, 11.36316805921044, 1.2870135924841382], [5.6489282516340396, 9.430140166559898, 10.486839725228455, 1.9440673295129438]]\n",
      "[[3.2046302403903617, 16.51320009171937, 14.815773993312334, 4.195241024517017], [3.0223932015042894, 14.058265704299508, 14.128443585210062, 2.955786262760578], [3.330855016117338, 15.306694710857982, 11.44057917507238, 1.2919196282660672], [4.657642411593202, 15.83749954078255, 15.382879095511548, 2.663166419090245]]\n"
     ]
    }
   ],
   "source": [
    "print(MVRR_model_GPs)\n",
    "print(NPS_model_GPs)\n",
    "print(NPZ_model_GPs)\n",
    "full_list = [MVRR_model_GPs, NPS_model_GPs, NPZ_model_GPs]\n",
    "gpath_cases = ['MVRR', 'NPS', 'NPZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8a7c399-58eb-4a5f-a578-a6ebe76982d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full_list)):\n",
    "    data = {\n",
    "        \"m0\": full_list[i][0],\n",
    "        \"m1\": full_list[i][1],\n",
    "        \"m2\": full_list[i][2],\n",
    "        \"m3\": full_list[i][3]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    title = gpath_cases[i] + \"_GPs.csv\"\n",
    "    path = os.path.join(base, title)\n",
    "    df.to_csv(path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bfe31-1081-4a46-8171-be97c3a56990",
   "metadata": {},
   "source": [
    "## Experiment 2:\n",
    "\n",
    "Training regressions on only 2 Garden Path cases, then used to produce reading times for the third GP case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f4ab6-2e78-40ef-932c-0e9ec2bc9ba4",
   "metadata": {},
   "source": [
    "##### Creating new GP only, training regression, predicting RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ae42373-274f-4183-9ac7-bf4b192f7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min = pd.read_csv(out_GP, header=0)\n",
    "gpath_cases = ['MVRR', 'NPS', 'NPZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d02a9-acfb-4745-ae9a-c91b2c7e5265",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvrr = [c for c in gpath_cases if c != 'MVRR']\n",
    "dropped_min_MVRR = dropped_min[dropped_min['Type'].isin([c + '_AMB' for c in mvrr] + [c + '_UAMB' for c in mvrr])]\n",
    "\n",
    "filler_model_syn = smf.mixedlm(\n",
    "    syn_model,\n",
    "    data=dropped_min_MVRR,\n",
    "    groups=dropped_min_MVRR[\"item\"],  \n",
    "    re_formula=\"1 + syn_surprisal_s + syn_surprisal_p1_s + syn_surprisal_p2_s\"\n",
    ").fit(method=\"lbfgs\", maxiter=100000, disp=True) \n",
    "\n",
    "dump(filler_model_syn, model_base + \"model-MVRR_onlyGP.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51496dce-45ad-4b25-ba98-14085569ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min_only_MVRR = dropped_min[dropped_min['Type'].isin(['MVRR_UAMB', 'MVRR_AMB'])]\n",
    "\n",
    "predictRT_dataset(syn_model, \n",
    "                  os.path.join(model_base, \"model-MVRR_onlyGP.joblib\"),\n",
    "                  dropped_min_only_MVRR, \n",
    "                  os.path.join(prediction_base, \"MVRR_pred_onlyGP.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b3b7e-627d-4b2a-8dff-84eaf5b6130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nps = [c for c in gpath_cases if c != 'NPS']\n",
    "dropped_min_NPS = dropped_min[dropped_min['Type'].isin([c + '_AMB' for c in nps] + [c + '_UAMB' for c in nps])]\n",
    "\n",
    "filler_model_syn = smf.mixedlm(\n",
    "    syntactic_model,\n",
    "    data=dropped_min_NPS,\n",
    "    groups=dropped_min_NPS[\"item\"],  \n",
    "    re_formula=\"1 + syn_surprisal_s + syn_surprisal_p1_s + syn_surprisal_p2_s\"\n",
    ").fit(method=\"lbfgs\", maxiter=100000, disp=True) \n",
    "\n",
    "dump(filler_model_syn, model_base + \"model-NPS_onlyGP.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfa7cd-cf7e-4fec-bf7a-c61f53e4550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min_only_NPS = dropped_min[dropped_min['Type'].isin(['NPS_UAMB', 'NPS_AMB'])]\n",
    "\n",
    "\n",
    "predictRT_dataset(syn_model, \n",
    "                  os.path.join(model_base, \"model-NPS_onlyGP.joblib\"),\n",
    "                  dropped_min_only_NPS, \n",
    "                  os.path.join(prediction_base, \"NPS_pred_onlyGP.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530e4d3-9ec5-4161-9d78-6343447fda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = [c for c in gpath_cases if c != 'NPZ']\n",
    "dropped_min_NPZ = dropped_min[dropped_min['Type'].isin([c + '_AMB' for c in npz] + [c + '_UAMB' for c in npz])]\n",
    "\n",
    "filler_model_syn = smf.mixedlm(\n",
    "    syntactic_model,\n",
    "    data=dropped_min_NPZ,\n",
    "    groups=dropped_min_NPZ[\"item\"],  \n",
    "    re_formula=\"1 + syn_surprisal_s + syn_surprisal_p1_s + syn_surprisal_p2_s\"\n",
    ").fit(method=\"lbfgs\", maxiter=100000, disp=True) \n",
    "\n",
    "dump(filler_model_syn, model_base + \"model-NPZ_onlyGP.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdb2a7-d1ea-4d38-8b8e-4aaae763bd2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_min_only_NPZ = dropped_min[dropped_min['Type'].isin(['NPZ_UAMB', 'NPZ_AMB'])]\n",
    "\n",
    "\n",
    "predictRT_dataset(syn_model, \n",
    "                  os.path.join(model_base, \"model-NPZ_onlyGP.joblib\"),\n",
    "                  dropped_min_only_NPZ, \n",
    "                  os.path.join(prediction_base, \"NPZ_pred_onlyGP.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d355f6-a61c-4323-b477-9be2010a923c",
   "metadata": {},
   "source": [
    "##### Inspecting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74a1029-7cf5-45f5-9870-3a587da28ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model at /Users/andrewperun/Desktop/LING_380/Final_Proj/Trial2/regressions/model-MVRR_onlyGP.joblib:\n",
      "Coefficients:\n",
      " Intercept                                      421.135621\n",
      "syn_surprisal_s                                 15.859941\n",
      "syn_surprisal_p1_s                              13.757483\n",
      "syn_surprisal_p2_s                               1.265022\n",
      "scale(WordPosition)                              1.559555\n",
      "logfreq_s                                      -21.896016\n",
      "length_s                                        15.666922\n",
      "logfreq_s:length_s                               0.387973\n",
      "logfreq_p1_s                                   -13.322270\n",
      "length_p1_s                                      3.437683\n",
      "logfreq_p1_s:length_p1_s                        -5.453208\n",
      "logfreq_p2_s                                   -12.884434\n",
      "length_p2_s                                     -7.777838\n",
      "logfreq_p2_s:length_p2_s                       -11.130806\n",
      "Group Var                                        0.001784\n",
      "Group x syn_surprisal_s Cov                      0.000475\n",
      "syn_surprisal_s Var                              0.000852\n",
      "Group x syn_surprisal_p1_s Cov                   0.000567\n",
      "syn_surprisal_s x syn_surprisal_p1_s Cov         0.000612\n",
      "syn_surprisal_p1_s Var                           0.001382\n",
      "Group x syn_surprisal_p2_s Cov                   0.000577\n",
      "syn_surprisal_s x syn_surprisal_p2_s Cov         0.000028\n",
      "syn_surprisal_p1_s x syn_surprisal_p2_s Cov      0.000329\n",
      "syn_surprisal_p2_s Var                           0.000516\n",
      "dtype: float64\n",
      "Model at /Users/andrewperun/Desktop/LING_380/Final_Proj/Trial2/regressions/model-NPS_onlyGP.joblib:\n",
      "Coefficients:\n",
      " Intercept                                      422.161312\n",
      "syn_surprisal_s                                 16.277253\n",
      "syn_surprisal_p1_s                              16.089093\n",
      "syn_surprisal_p2_s                               1.592158\n",
      "scale(WordPosition)                              1.888670\n",
      "logfreq_s                                      -24.488403\n",
      "length_s                                        19.372046\n",
      "logfreq_s:length_s                               0.525275\n",
      "logfreq_p1_s                                   -15.070157\n",
      "length_p1_s                                      7.562569\n",
      "logfreq_p1_s:length_p1_s                        -4.149979\n",
      "logfreq_p2_s                                   -10.838881\n",
      "length_p2_s                                     -6.192776\n",
      "logfreq_p2_s:length_p2_s                        -9.409102\n",
      "Group Var                                        0.001435\n",
      "Group x syn_surprisal_s Cov                     -0.000046\n",
      "syn_surprisal_s Var                              0.000344\n",
      "Group x syn_surprisal_p1_s Cov                   0.000253\n",
      "syn_surprisal_s x syn_surprisal_p1_s Cov         0.000234\n",
      "syn_surprisal_p1_s Var                           0.000838\n",
      "Group x syn_surprisal_p2_s Cov                   0.000285\n",
      "syn_surprisal_s x syn_surprisal_p2_s Cov        -0.000122\n",
      "syn_surprisal_p1_s x syn_surprisal_p2_s Cov      0.000299\n",
      "syn_surprisal_p2_s Var                           0.000675\n",
      "dtype: float64\n",
      "Model at /Users/andrewperun/Desktop/LING_380/Final_Proj/Trial2/regressions/model-NPZ_onlyGP.joblib:\n",
      "Coefficients:\n",
      " Intercept                                      417.077374\n",
      "syn_surprisal_s                                 13.035634\n",
      "syn_surprisal_p1_s                              11.229804\n",
      "syn_surprisal_p2_s                               1.242798\n",
      "scale(WordPosition)                              0.932380\n",
      "logfreq_s                                      -22.511440\n",
      "length_s                                        17.695778\n",
      "logfreq_s:length_s                              -1.125067\n",
      "logfreq_p1_s                                   -12.757100\n",
      "length_p1_s                                      6.965643\n",
      "logfreq_p1_s:length_p1_s                        -5.318830\n",
      "logfreq_p2_s                                    -9.260338\n",
      "length_p2_s                                     -5.365595\n",
      "logfreq_p2_s:length_p2_s                        -8.172335\n",
      "Group Var                                        0.001272\n",
      "Group x syn_surprisal_s Cov                      0.000347\n",
      "syn_surprisal_s Var                              0.000685\n",
      "Group x syn_surprisal_p1_s Cov                   0.000347\n",
      "syn_surprisal_s x syn_surprisal_p1_s Cov         0.000389\n",
      "syn_surprisal_p1_s Var                           0.001589\n",
      "Group x syn_surprisal_p2_s Cov                   0.000308\n",
      "syn_surprisal_s x syn_surprisal_p2_s Cov        -0.000114\n",
      "syn_surprisal_p1_s x syn_surprisal_p2_s Cov      0.000513\n",
      "syn_surprisal_p2_s Var                           0.001056\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "inspect_model(os.path.join(model_base, \"model-MVRR_onlyGP.joblib\"))\n",
    "inspect_model(os.path.join(model_base, \"model-NPS_onlyGP.joblib\"))\n",
    "inspect_model(os.path.join(model_base, \"model-NPZ_onlyGP.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6b4f80-2880-4c6b-a36f-e84c0d2c7886",
   "metadata": {},
   "source": [
    "##### Getting GP effect from new RT predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c253f53c-16db-41b3-9343-eba37d26cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "MVRR_preds_2 = split_predictions(os.path.join(prediction_base, \"MVRR_pred_onlyGP.csv\"))\n",
    "NPS_preds_2  = split_predictions(os.path.join(prediction_base, \"NPS_pred_onlyGP.csv\"))\n",
    "NPZ_preds_2  = split_predictions(os.path.join(prediction_base, \"NPZ_pred_onlyGP.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "84a5f986-b43e-4086-9a9a-6740d0f2eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MVRR_human_GP_2, MVRR_model_GPs_2 = get_GPeffect_byModel(MVRR_preds_2, lagged_sep, 'mvrr')\n",
    "NPS_human_GP_2,  NPS_model_GPs_2  = get_GPeffect_byModel(NPS_preds_2,  lagged_sep, 'nps')\n",
    "NPZ_human_GP_2,  NPZ_model_GPs_2  = get_GPeffect_byModel(NPZ_preds_2,  lagged_sep, 'npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ed8204c2-e58c-4db1-a370-0ab58833d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.5711756860257005, 41.737653392294625, 167.4356757427677, 94.73673452609182]\n",
      "[17.100729397502587, 32.337040211112644, 49.61063704069727, 18.32235934651217]\n",
      "[5.757101983650362, 84.22053326940727, 119.79536401912738, 51.919479000115764]\n"
     ]
    }
   ],
   "source": [
    "# Should be same as human GP in experiment 1\n",
    "print(MVRR_human_GP_2)\n",
    "print(NPS_human_GP_2)\n",
    "print(NPZ_human_GP_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "997f3567-74f0-4661-986f-6e87d5934b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.339772367011326, 20.10839685793695, 27.018904114778778, 11.247445301578226], [-7.820421732195722, 14.775854930736758, 20.509725451776347, 8.29096741024082], [-5.335067435438879, 18.25203295549088, 20.770113294755795, 7.121386093520472], [-15.803930798748292, 24.266202118193117, 34.32271181381055, 13.14833135207386]]\n",
      "[[9.67057455685505, 16.42639785666187, 22.85179183211333, 11.216716922543617], [7.45254369961437, 17.103169266332824, 26.54465374534607, 13.84108675351633], [7.63433747870949, 19.086659276201924, 23.943154219220407, 8.263084374563816], [7.647498575587257, 12.743781432423235, 19.700115936424215, 8.505523838181348]]\n",
      "[[-3.676067337020094, 20.150581982951255, 27.046147552583204, 15.14101262643694], [-2.9939768998268357, 17.284214846857008, 24.90060068653592, 12.951612196823186], [-1.4802614735945676, 19.168520552500073, 22.353437304213475, 9.692395835927428], [1.4432907334748295, 20.982119660165854, 27.80828441714456, 13.560491733887142]]\n"
     ]
    }
   ],
   "source": [
    "print(MVRR_model_GPs_2)\n",
    "print(NPS_model_GPs_2)\n",
    "print(NPZ_model_GPs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7f3920-edbf-45fa-b235-fd22ded4e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list_2 = [MVRR_model_GPs_2, NPS_model_GPs_2, NPZ_model_GPs_2]\n",
    "gpath_cases = ['MVRR', 'NPS', 'NPZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0dfcbae-202e-4ba0-9f27-b351341ac593",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full_list_2)):\n",
    "    data = {\n",
    "        \"m0\": full_list_2[i][0],\n",
    "        \"m1\": full_list_2[i][1],\n",
    "        \"m2\": full_list_2[i][2],\n",
    "        \"m3\": full_list_2[i][3]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    title = gpath_cases[i] + \"_GPs_2.csv\"\n",
    "    path = os.path.join(base, title)\n",
    "    df.to_csv(path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b21f5e-5f3a-4bbe-a1ce-70916c23d0db",
   "metadata": {},
   "source": [
    "## Prediction of RT & GP Effect of in-sample data (Experiment 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3180552c-47aa-428b-ba25-159e69082ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_predictions_2(preds_path):\n",
    "\n",
    "    df = pd.read_csv(preds_path)\n",
    "    split = {}\n",
    "\n",
    "    # iter through construction type\n",
    "    for col_2_value in np.unique(df.iloc[:, 2]): \n",
    "        filtered_col_2 = df[df.iloc[:, 2] == col_2_value]\n",
    "        split[col_2_value] = {}\n",
    "\n",
    "        #  iter through model type\n",
    "        for col_18_value in np.unique(filtered_col_2.iloc[:, 18]):\n",
    "            filtered_col_18 = filtered_col_2[filtered_col_2.iloc[:, 18] == col_18_value]\n",
    "            split[col_2_value][col_18_value] = {}\n",
    "\n",
    "            # iter through item number (sample sentence ID)\n",
    "            for col_17_value in np.unique(filtered_col_18.iloc[:, 17]):\n",
    "                filtered_col_17 = filtered_col_18[filtered_col_18.iloc[:, 17] == col_17_value]\n",
    "                split[col_2_value][col_18_value][col_17_value] = filtered_col_17\n",
    "\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2307515-fbe8-4f16-8a38-f1f05c6d2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GPeffect_inSample(predictions, lagged_truth, const):\n",
    "\n",
    "    GP_effects = []\n",
    "    for i in predictions[const.upper() + '_UAMB']['m0'].keys(): # for each sentence\n",
    "        \n",
    "        i_list = []\n",
    "        \n",
    "        # get word position of disambiguating word\n",
    "        truth = lagged_truth[i][const][const.upper() + '_UAMB']['m0']\n",
    "        ambig_disambig = truth['disambPositionAmb'].iloc[0]\n",
    "        unambig_disambig = truth['disambPositionUnamb'].iloc[0]\n",
    "        #print(f\"Item {i}: {ambig_disambig}\")\n",
    "        #print(f\"Item {i}: {unambig_disambig}\")\n",
    "\n",
    "\n",
    "\n",
    "        for model in ['m0', 'm1', 'm2', 'm3']:\n",
    "\n",
    "            m_list = []\n",
    "\n",
    "            for j in [-1, 0, 1, 2]:\n",
    "                \n",
    "                unamb =  predictions[const.upper() + '_UAMB'][model][i]#.iloc[:,9] # 9 indexes word position\n",
    "                amb  =  predictions[const.upper() + '_AMB'][model][i]#.iloc[:,9] \n",
    "\n",
    "                df1 = unamb[unamb.iloc[:,9] == j + unambig_disambig].iloc[:,21] # 21 column indexes new predicted RT\n",
    "                df2 = amb[amb.iloc[:,9]  == j + ambig_disambig].iloc[:,21]\n",
    "                #min_len = min([len(df1), len(df2)])\n",
    "\n",
    "                df1 = df1.reset_index(drop=True)\n",
    "                df2 = df2.reset_index(drop=True)\n",
    "\n",
    "                pred_unamb_rts = df1[:].apply(lambda x: float(x[-25:-18])).mean()\n",
    "                \n",
    "                pred_amb_rts   = df2[:].apply(lambda x: float(x[-25:-18])).mean()\n",
    "                #print(\"Uamb: \" + str(pred_unamb_rts))\n",
    "                #print(\"Amb: \" + str(pred_amb_rts))\n",
    "                \n",
    "                diffs = (pred_amb_rts-pred_unamb_rts).tolist()\n",
    "                #print(diffs)\n",
    "                m_list.append(np.mean(diffs))\n",
    "\n",
    "            i_list.append(m_list)\n",
    "            \n",
    "        GP_effects.append(i_list)\n",
    "\n",
    "    #print(GP_effects)\n",
    "    trans = list(zip(*GP_effects))\n",
    "    means = []\n",
    "    for group in trans:\n",
    "        mean_sublist = np.mean(group, axis=0)\n",
    "        means.append(mean_sublist)\n",
    "\n",
    "    model_GP_effects = [list(sublist) for sublist in means]\n",
    "    \n",
    "    return model_GP_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "733b7988-f353-4436-8eb7-86c5dc13c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_save(GP_effect_list, out_path, col1, col2):\n",
    "    \n",
    "    averages_A = [sum(x) / len(x) for x in zip(*GP_effect_list[0])]\n",
    "    averages_B = [sum(x) / len(x) for x in zip(*GP_effect_list[1])]\n",
    "    \n",
    "    df = pd.DataFrame({col1: averages_A, col2: averages_B})\n",
    "    df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcec1f9e-019c-4706-9d28-8f98108f3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min = pd.read_csv(out_GP, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cefb2d4e-f1c6-43f1-8253-76c04200c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_min_MVRR_NPS = dropped_min[dropped_min['Type'].isin(['MVRR_AMB', 'NPS_AMB', 'MVRR_UAMB', 'NPS_UAMB'])]\n",
    "dropped_min_MVRR_NPZ = dropped_min[dropped_min['Type'].isin(['MVRR_AMB', 'NPZ_AMB', 'MVRR_UAMB', 'NPZ_UAMB'])]\n",
    "dropped_min_NPS_NPZ  = dropped_min[dropped_min['Type'].isin(['NPS_AMB', 'NPZ_AMB', 'NPS_UAMB', 'NPZ_UAMB'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1bc157f1-1f21-4507-bde3-e19fedcbbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_model = \"\"\"\n",
    "    RT ~ syn_surprisal_s + syn_surprisal_p1_s +\n",
    "         syn_surprisal_p2_s + scale(WordPosition) +\n",
    "         logfreq_s * length_s + logfreq_p1_s * length_p1_s +\n",
    "         logfreq_p2_s * length_p2_s\n",
    "    \"\"\"\n",
    "\n",
    "random_effect = \"1 + syn_surprisal_s + syn_surprisal_p1_s + syn_surprisal_p2_s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d6c8e7e3-77d2-4b9d-8e32-7d3195f04c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_27910/1558022743.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"fe_prediction\"] = fe_matrix @ fe_params\n",
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_27910/1558022743.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"rand_effects\"] = dataset[\"item\"].map(rand_effects).fillna(0)\n",
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_27910/1558022743.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"predicted_rt_syn\"] = dataset[\"fe_prediction\"] + dataset[\"rand_effects\"] # get final RT predictions\n"
     ]
    }
   ],
   "source": [
    "predictRT_dataset(syn_model, \n",
    "                  os.path.join(model_base, \"model-MVRR_onlyGP.joblib\"),\n",
    "                  dropped_min_NPS_NPZ, \n",
    "                  os.path.join(prediction_base, \"ex2_insample_NPS_NPZ_pred_onlyGP.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e28119c5-4e7a-4ecc-aaf4-cbc2d0e611e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_27910/1558022743.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"fe_prediction\"] = fe_matrix @ fe_params\n",
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_27910/1558022743.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"rand_effects\"] = dataset[\"item\"].map(rand_effects).fillna(0)\n",
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_27910/1558022743.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"predicted_rt_syn\"] = dataset[\"fe_prediction\"] + dataset[\"rand_effects\"] # get final RT predictions\n"
     ]
    }
   ],
   "source": [
    "predictRT_dataset(syn_model, \n",
    "                  os.path.join(model_base, \"model-NPS_onlyGP.joblib\"),\n",
    "                  dropped_min_MVRR_NPZ, \n",
    "                  os.path.join(prediction_base, \"ex2_insample_MVRR_NPZ_pred_onlyGP.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24e3b95d-42e0-4e76-b4c3-17147068887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_55479/1558022743.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"fe_prediction\"] = fe_matrix @ fe_params\n",
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_55479/1558022743.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"rand_effects\"] = dataset[\"item\"].map(rand_effects).fillna(0)\n",
      "/var/folders/cp/13y3b68j0tz9h3vtqsf6lb400000gn/T/ipykernel_55479/1558022743.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset[\"predicted_rt_syn\"] = dataset[\"fe_prediction\"] + dataset[\"rand_effects\"] # get final RT predictions\n"
     ]
    }
   ],
   "source": [
    "predictRT_dataset(syn_model, \n",
    "                  os.path.join(model_base, \"model-NPZ_onlyGP.joblib\"),\n",
    "                  dropped_min_MVRR_NPS,\n",
    "                  os.path.join(prediction_base, \"ex2_insample_MVRR_NPS_pred_onlyGP.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba59d729-8fab-4486-b61c-008ce2dbd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_preds_NPS_NPZ  = split_predictions_2(os.path.join(prediction_base, \"ex2_insample_NPS_NPZ_pred_onlyGP.csv\"))\n",
    "sep_preds_MVRR_NPZ = split_predictions_2(os.path.join(prediction_base, \"ex2_insample_MVRR_NPZ_pred_onlyGP.csv\"))\n",
    "sep_preds_MVRR_NPS = split_predictions_2(os.path.join(prediction_base, \"ex2_insample_MVRR_NPS_pred_onlyGP.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be592fa8-2692-437b-b3f6-cc8a72105337",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "        'lex_surprisal_s', 'lex_surprisal_p1_s', 'lex_surprisal_p2_s', 'lex_surprisal_p3_s',\n",
    "        'syn_surprisal_s', 'syn_surprisal_p1_s', 'syn_surprisal_p2_s', 'syn_surprisal_p3_s',\n",
    "        'logfreq_s', 'logfreq_p1_s', 'logfreq_p2_s', 'logfreq_p3_s']\n",
    "\n",
    "lagged_sep = split_Lags(os.path.join(base, \"with_lags.csv\"), columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de024c2e-f650-4e8c-963d-d64d7dedda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_effect_MVRR_1 = get_GPeffect_inSample(sep_preds_MVRR_NPZ, lagged_sep, 'mvrr') # model A\n",
    "GP_effect_MVRR_2 = get_GPeffect_inSample(sep_preds_MVRR_NPS, lagged_sep, 'mvrr') # model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "baf9edfd-fa8f-4716-a14f-58177ec1cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_effect_NPS_1 = get_GPeffect_inSample(sep_preds_MVRR_NPS, lagged_sep, 'nps') # model B\n",
    "GP_effect_NPS_2 = get_GPeffect_inSample(sep_preds_NPS_NPZ, lagged_sep, 'nps') # model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01b0303a-ffc4-4127-8535-bb853405849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_effect_NPZ_1 = get_GPeffect_inSample(sep_preds_MVRR_NPZ, lagged_sep, 'npz') # model A\n",
    "GP_effect_NPZ_2 = get_GPeffect_inSample(sep_preds_NPS_NPZ, lagged_sep, 'npz') # model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "52285904-e94b-4e66-9743-010caad5c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MVRR_out = os.path.join(base, \"MVRR_control_2.csv\")\n",
    "NPS_out =  os.path.join(base, \"NPS_control_2.csv\")\n",
    "NPZ_out =  os.path.join(base, \"NPZ_control_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31de9f9e-baf2-4e96-a0f9-81b0faf98205",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_save([GP_effect_MVRR_1, GP_effect_MVRR_2], MVRR_out, 'A', 'B')\n",
    "get_avg_save([GP_effect_NPS_1, GP_effect_NPS_2], NPS_out, 'B', 'C')\n",
    "get_avg_save([GP_effect_NPZ_1, GP_effect_NPZ_2], NPZ_out, 'A', 'C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233536f-109e-4b17-ab70-b8f1f391aa99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
